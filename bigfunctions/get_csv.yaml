type: function_py
category: get_data
author:
  name: Javier de la Torre
  url: https://www.linkedin.com/in/jatorre/
  avatar_url: "https://media.licdn.com/dms/image/v2/D5603AQGIcLL8AHJQmA/profile-displayphoto-shrink_800_800/profile-displayphoto-shrink_800_800/0/1683558695365?e=1732752000&v=beta&t=2u7VbR1MQyWmO5aDln36dt-8lwa1mo4E1sbM0AAv3kA"
description: |
  Download csv file from `url` into `destination_table`

  > **Requirements**
  >
  > You must create the `destination_dataset` and give `dataEditor` access to `bigfunction@bigfunctions.iam.gserviceaccount.com` before calling this function.
  > You can do this by executing:
  >
  > ```sql
  > -- Create Destination Dataset
  > create schema `your_project.your_dataset`;
  >
  > -- Grant Access to Destination Dataset
  > grant `roles/bigquery.dataEditor`
  > on schema `your_project.your_dataset`
  > to 'serviceAccount:bigfunction@bigfunctions.iam.gserviceaccount.com';
  > ```
arguments:
  - name: url
    type: string
  - name: destination_table
    type: string
output:
  name: status
  type: string
examples:
  - description: ""
    arguments:
      - "'https://firms.modaps.eosdis.nasa.gov/data/active_fire/modis-c6.1/csv/MODIS_C6_1_Global_7d.csv'"
      - "'your_project.your_dataset.your_table'"
    output: "ok"
code: |
  if not url:
    return 'invalid url: it is null or empty'
  if not url.startswith(('http://', 'https://')):
    return 'invalid url: it does not start with http:// nor https://'

  import pandas as pd
  import google.cloud.bigquery
  import google.api_core.exceptions

  bigquery = google.cloud.bigquery.Client()

  df = pd.read_csv(url)
  try:
    bigquery.load_table_from_dataframe(df, destination_table).result()
  except (google.api_core.exceptions.Forbidden, google.api_core.exceptions.NotFound, google.api_core.exceptions.PermissionDenied) as e:
    assert False, f'Service Account `{get_current_service_account()}` does not have data-editor permission for given destination dataset (or the dataset does not exsit). Please add it'
  return 'ok'
requirements: |
  pandas
  pyarrow
  google-cloud-bigquery
quotas:
  max_rows_per_query: 1
cloud_run:
  memory: 1024Mi
  concurrency: 1
  max_instances: 10


