type: function_py
category: get_data
author:
  name: Paul Marcombes
  url: https://www.linkedin.com/in/paul-marcombes
  avatar_url: "https://lh3.googleusercontent.com/a-/ACB-R5RDf2yxcw1p_IYLCKmiUIScreatDdhG8B83om6Ohw=s260"
description: |
  Get data from 250+ sources using [Airbyte Python Connectors](https://docs.airbyte.com/using-airbyte/pyairbyte/getting-started#available-connectors)
  .

  > Data is extracted from `source` using `source_config` and appended in raw format in stream tables (one table per stream) into `destination_dataset`.
  > When supported by the stream, data is extracted incrementally (next execution will only retrieve new rows). For this, a state is stored in `_airbyte_states` table in `destination_dataset`.
  >
  > You must create the `destination_dataset` and give `dataEditor` access to `bigfunction@bigfunctions.iam.gserviceaccount.com` before calling this function.
  > You can do this by executing:
  >
  > ```sql
  > -- Create Destination Dataset
  > create schema `your_project.your_dataset`;
  >
  > -- Grant Access to Destination Dataset
  > grant `roles/bigquery.dataEditor`
  > on schema `your_project.your_dataset`
  > to 'serviceAccount:bigfunction@bigfunctions.iam.gserviceaccount.com';
  > ```
  >
  > While it's running (or after) you can explore logs in table `_airbyte_logs` in `destination_dataset`.
  >
  > *This functions uses [Airbyte-Serverless](https://github.com/unytics/airbyte_serverless)*.

arguments:
  - name: source
    type: string
  - name: source_config
    type: string
  - name: streams
    type: string
  - name: destination_dataset
    type: string
output:
  name: result
  type: string
examples:
  - description: "List sources available for `source` argument by setting `source` to `null`"
    arguments:
      - "null"
      - "null"
      - "null"
      - "null"
    output: |
      # AVAILABLE SOURCES

      airbyte-source-activecampaign==0.1.10
      airbyte-source-adjust==0.1.11
      airbyte-source-aha==0.3.10
      ...
  - description: |
      Get `source_config` sample at expected format by setting `source_config` to `null`.

      You can then copy the result, modify it and provide it as `source_config` argument.
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - "null"
      - "null"
      - "null"
    output: |
      # SOURCE CONFIG

      id: # OPTIONAL | string
      options: # OPTIONAL | string
  - description: "List available streams by setting `streams` param to null"
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - |
        '''
        id: "1234"
        '''
      - "null"
      - "null"
    output: |
      # AVAILABLE STREAMS

      launches, capsules, company, crew, cores, dragons, landpads, payloads, history, rockets, roadster, ships, starlink
  - description: "Extract and load `crew` and `rockets` data incrementally into `destination_dataset`"
    arguments:
      - "'airbyte-source-spacex-api==0.1.11'"
      - |
        '''
        id: "1234"
        '''
      - "'crew, rockets'"
      - "'your_project.your_dataset'"
    output: ok
init_code: |
  import requests
  import yaml
  from airbyte_serverless.connections import Connection
  from airbyte_serverless.sources import ExecutableAirbyteSource
  from airbyte_serverless import airbyte_utils
  import google.api_core.exceptions


  SOURCES_URL = 'https://connectors.airbyte.com/files/registries/v0/oss_registry.json'


  def get_sources():
    resp = requests.get(SOURCES_URL)
    res = resp.json()
    sources = res['sources']
    python_sources = [
      f"{source.get('remoteRegistries', {}).get('pypi', {}).get('packageName')}=={source['dockerImageTag']}"
      for source in sources
      if source.get('remoteRegistries', {}).get('pypi', {}).get('enabled') is True
    ]
    return python_sources


  def get_yaml_definition_example(executable):
    airbyte_source =  ExecutableAirbyteSource(executable)
    spec = airbyte_source.spec
    yaml_config = airbyte_utils.generate_connection_yaml_config_sample(spec)
    if yaml_config.startswith('#'):
      yaml_config = '\n'.join(yaml_config.split('\n')[1:])
    return yaml_config


  def yaml2dict(source_yaml_config):
    try:
      return yaml.safe_load(source_yaml_config) or {}
    except:
      assert False, 'Given `source_config` is NOT a valid yaml content'


  def list_streams(executable, source_config):
    airbyte_source = ExecutableAirbyteSource(executable=executable, config=source_config)
    return airbyte_source.available_streams


  def run(executable, source_config, streams, destination_dataset):
    config = {
      'source': {
        'executable': executable,
        'config': source_config,
        'streams': streams,
      },
      'destination': {
        'connector': 'bigquery',
        'config': {
          'dataset': destination_dataset,
          'buffer_size_max': 1000,
        },
      },
    }
    yaml_config = yaml.dump(config)
    connection = Connection(yaml_config)
    try:
      connection.run()
    except (google.api_core.exceptions.Forbidden, google.api_core.exceptions.NotFound, google.api_core.exceptions.PermissionDenied) as e:
      assert False, f'Service Account `{get_current_service_account()}` does not have data-editor permission for given destination dataset (or the dataset does not exsit). Please add it'


  AVAILABLE_SOURCES = get_sources()

code: |
  source = source or ''
  source_is_unavailable = not source or not [s for s in AVAILABLE_SOURCES if s.split('==')[0] == source.split('==')[0]]
  if source_is_unavailable:
    return '# AVAILABLE SOURCES\n\n' + '\n'.join(AVAILABLE_SOURCES)

  executable = f'pipx run {source}'
  source_config = source_config or ''
  if not source_config.strip():
    source_config = get_yaml_definition_example(executable)
    return '# SOURCE CONFIG\n\n' + source_config

  source_config = yaml2dict(source_config)

  if not streams:
    streams = list_streams(executable, source_config)
    return '# AVAILABLE STREAMS\n\n' + ', '.join(streams)

  run(executable, source_config, streams, destination_dataset)
  return 'ok'
dockerfile:
  image: ubuntu:22.04
  apt_packages: python3.10-venv python3-pip
requirements: |
  airbyte-serverless
max_batching_rows: 1
quotas:
  max_rows_per_user_per_day: 200
cloud_run:
  memory: 512Mi
  max_instances: 10
  concurrency: 1
  timeout: 30m
